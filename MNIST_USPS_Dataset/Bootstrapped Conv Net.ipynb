{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapped Conv NN\n",
    "\n",
    "A bootstrapped approach to domain adaptation for MNIST to USPS.\n",
    "\n",
    "Author: @ysbecca\n",
    "\n",
    "Credits go to:\n",
    "\n",
    "- [Hvass-Labs](http://www.hvass-labs.org/) for the base CNN model for MNIST classification. Their CNN was deepened and adjusted to iteratively learn the USPS dataset.\n",
    "- [Leo Pauly](https://github.com/leopauly) for the research in general and the initial code which loads the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from importlib import reload\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Custom scripts.\n",
    "import ysb\n",
    "import dataset\n",
    "import mnist_usps as mnus\n",
    "import cnn_helper as cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset' from '/Users/ysbecca/ysbecca-projects/Domain-Adaptation/MNIST_USPS_Dataset/dataset.py'>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mnus)\n",
    "reload(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN PARAMETERS\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "filter_size2 = 5         \n",
    "num_filters2 = 16    \n",
    "\n",
    "filter_size3 = 5         \n",
    "num_filters3 = 32    \n",
    "\n",
    "fc_size = 128             # Number of neurons in fully-connected layer.\n",
    "\n",
    "# IMAGE PARAMETERS\n",
    "img_size = 16             # Width and height in pixels.\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "data = dataset.read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset:\n",
      "- Training-set:\t\t49000\n",
      "- Test-set:\t\t14000\n",
      "- Validation-set:\t7000\n",
      "USPS dataset:\n",
      "- Training-set:\t\t7439\n",
      "- Test-set:\t\t1859\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST dataset:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(data.mnist_train.num_images))\n",
    "print(\"- Test-set:\\t\\t{}\".format(data.mnist_test.num_images))\n",
    "print(\"- Validation-set:\\t{}\".format(data.mnist_valid.num_images))\n",
    "\n",
    "print(\"USPS dataset:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(data.usps_train.num_images))\n",
    "print(\"- Test-set:\\t\\t{}\".format(data.usps_test.num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# ysb.show_labeled_patches_gray(data.mnist_train.images[i:], data.mnist_train.cls[i:])\n",
    "# ysb.show_labeled_patches_gray(data.usps_train.images[i:], data.usps_train.cls[i:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN on MNIST.\n",
    "\n",
    "First, set up the placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1) # A placeholder only, so calculated instead of passed directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build TensorFlow graph for a simple 3-layer CNN with max-pooling, ReLU and two fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    cn.new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True,\n",
    "                   w_name=\"conv1_w\")\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    cn.new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True,\n",
    "                   w_name=\"conv2_w\")\n",
    "layer_conv3, weights_conv3 = \\\n",
    "    cn.new_conv_layer(input=layer_conv2,\n",
    "                   num_input_channels=num_filters2,\n",
    "                   filter_size=filter_size3,\n",
    "                   num_filters=num_filters3,\n",
    "                   use_pooling=True,\n",
    "                   w_name=\"conv3_w\")\n",
    "layer_flat, num_features = cn.flatten_layer(layer_conv3)\n",
    "layer_fc1 = cn.new_fc_layer(input=layer_flat,                # num_features = 128, from the flattened 3rd layer.\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True,\n",
    "                         w_name=\"fc1_w\")\n",
    "layer_fc2 = cn.new_fc_layer(input=layer_fc1,                 # This is the classification layer.\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,            # num_classes = 10\n",
    "                         use_relu=False,\n",
    "                         w_name=\"fc2_w\")\n",
    "y_pred = tf.nn.softmax(layer_fc2)                            # This is the softmax output prediction.\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)                  # The hard classification, with a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost to be reduced is the the cross entropy over all the images.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new TensorFlow session and initialise the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for running optimisation iterations and showing test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    global total_iterations\n",
    "    start_time = time.time()\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    \n",
    "def print_test_accuracy(test_set, show_confusion_matrix=False):\n",
    "    num_test = len(test_set)\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images and targets from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "        labels = data.test.labels[i:j, :]\n",
    "        feed_dict = {x: images, y_true: labels}\n",
    "\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "    correct_sum = correct.sum()\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on test set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "    \n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion matrix:\")\n",
    "        cn.plot_confusion_matrix(data_cls_true, cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In progress: write optimisation as a function of the DataSet class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cn.plot_confusion_matrix(data_cls_true, cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test on USPS training set, and display accuracy as well as percentage of results by softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In progress: write a function for the DataSet class which adds given images to the training set and targets, \n",
    "# re-shuffles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WHILE the accuracy of the MNIST test set does NOT decrease, continue bootstrapping. Display results and save\n",
    "# predictions every round to a .csv.\n",
    "# cn.write_test_predictions\n",
    "# set_id = {0: MNIST, 1: USPS}\n",
    "# Save results to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy over bootstrap iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
